
\subsection*{Observations}

to write

\subsection*{Computation of Gradients}

Computation of the gradient of the function in equation (36) in \citep{Yi_2019}

Let
$$g(x_i) = f_i(x_i, \mathbf{x}_{-i}) = c_i(x_i) - (P(Ax))^T A_ix_i,$$
where $1 \leqslant i \leqslant N$, $x_i \in \mathbb{R}^{n_i}$, $c_i: \mathbb{R}^{n_i} \supset \Omega_i \to \mathbb{R}$ is the local production cost function, $P: \mathbb{R}^m \to \mathbb{R}^m$ maps the total supply of each market to its corresponding price, $A = [A_1, \ldots, A_N]$, $x = \operatorname{col}(x_1, \ldots, x_N)$, $Ax = \sum\limits_{1 \leqslant j \leqslant N} A_jx_j$. 

Let $p_i: \mathbb{R}^{n_i} \to \mathbb{R}^m$ be the function of supply of the $i$-th company to the markets, i.e. $p_i(x_i) = P(Ax) = P\left(\sum\limits_{1 \leqslant j \leqslant N} A_jx_j\right)$. Then
\begin{align*}
\operatorname{grad} g(x_i) & = \nabla_{x_i} f_i(x_i, \mathbf{x}_{-i}) = \operatorname{grad} c_i(x_i) - \left( \dfrac{\partial \left( (p_i(x_i))^T A_ix_i \right)}{\partial (x_i)_k} \right)_{k=1}^{n_i} \\
& = \operatorname{grad} c_i(x_i) - \left( \dfrac{\partial \left( \sum\limits_{1\leqslant t \leqslant m} (p_i(x_i))_t (A_ix_i)_t \right)}{\partial (x_i)_k} \right)_{k=1}^{n_i} \\
& = \operatorname{grad} c_i(x_i) - \left( \sum\limits_{1\leqslant t \leqslant m}\left( \dfrac{\partial \left( (p_i(x_i))_t \right)}{\partial (x_i)_k} (A_ix_i)_t + \dfrac{\partial \left( (A_ix_i)_t \right)}{\partial (x_i)_k} (p_i(x_i))_t \right) \right)_{k=1}^{n_i} \\
% & = \operatorname{grad} c_i(x_i) - \left( \sum\limits_{1\leqslant t \leqslant m}\left( \left(\operatorname{Jac} (p_i(x_i)) \right)_{tk} (A_ix_i)_t + \dfrac{\partial \left( \sum\limits_{s} (A_i)_{ts} (x_i)_s \right)}{\partial (x_i)_k} (p_i(x_i))_t \right) \right)_{k=1}^{n_i} \\
& = \operatorname{grad} c_i(x_i) - \left( \sum\limits_{1\leqslant t \leqslant m}\left( \left(\operatorname{Jac} (p_i)(x_i) \right)_{tk} (A_ix_i)_t + (A_i)_{tk} (p_i(x_i))_t \right) \right)_{k=1}^{n_i} \\
& = \operatorname{grad} c_i(x_i) - \left( \langle \operatorname{Jac}(p_i)(x_i)_{[:,k]}, A_ix_i \rangle + \langle (A_i)_{[:,k]}, p_i(x_i) \rangle \right)_{k=1}^{n_i} \\
& =\operatorname{grad} c_i(x_i) - \left( \operatorname{Jac}(p_i)(x_i) \right)^T A_ix_i - A_i^T p_i(x_i),
\end{align*}
with $\operatorname{Jac}(p_i)(x_i) = \left( \operatorname{Jac}(P)\left(\sum\limits_j A_jx_j\right) \right) \cdot A_i$.

% Or equivalently,

% \begingroup
% \allowdisplaybreaks
% \begin{align*}
% & \operatorname{grad} g(x_i) = \nabla_{x_i} f_i(x_i, \mathbf{x}_{-i}) = \operatorname{grad} c_i(x_i) - \left( \dfrac{\partial \left( (P(Ax))^T A_ix_i \right)}{\partial (x_i)_k} \right)_{k=1}^{n_i} \\
% = & \operatorname{grad} c_i(x_i) - \left( \dfrac{\partial \left( \sum\limits_{1\leqslant t \leqslant m} P_t(Ax) (A_ix_i)_t \right)}{\partial (x_i)_k} \right)_{k=1}^{n_i} \\
% = & \operatorname{grad} c_i(x_i) - \left( \sum\limits_{1\leqslant t \leqslant m} \left[ \dfrac{\partial P_t \left( \sum\limits_j A_jx_j \right)}{\partial (x_i)_k} (A_ix_i)_t + P_t \left( \sum\limits_j A_jx_j \right) \dfrac{\partial (A_ix_i)_t}{\partial (x_i)_k} \right] \right)_{k=1}^{n_i} \\
% = & \operatorname{grad} c_i(x_i) - \left( \sum\limits_{1\leqslant t \leqslant m} \left[ \left( \sum\limits_{1\leqslant h \leqslant m} \partial_h P_t\left( \sum\limits_j A_jx_j \right) \dfrac{\partial \left( \sum\limits_j A_jx_j \right)_h}{\partial (x_i)_k} \right) (A_ix_i)_t \right.\right. \\
% & \left.\left. \hspace{16em} + P_t \left( \sum\limits_j A_jx_j \right) \dfrac{\partial \left( \sum\limits_{s} (A_i)_{ts} (x_i)_s \right)}{\partial (x_i)_k} \right] \right)_{k=1}^{n_i} \\
% = & \operatorname{grad} c_i(x_i) - \left( \sum\limits_{1\leqslant t \leqslant m} \left[ \left( \sum\limits_{1\leqslant h \leqslant m} \partial_h P_t\left( \sum\limits_j A_jx_j \right) \dfrac{\partial \left( \sum\limits_j\sum\limits_s (A_j)_{hs} (x_j)_s \right)}{\partial (x_i)_k} \right) \sum\limits_s (A_i)_{ts} (x_i)_s \right.\right. \\
% & \left.\left. \hspace{24em} + P_t \left( \sum\limits_j A_jx_j \right) (A_i)_{tk} \right] \right)_{k=1}^{n_i} \\
% = & \operatorname{grad} c_i(x_i) - \left( \sum\limits_{1\leqslant t \leqslant m} \left[ \left( \sum\limits_{1\leqslant h \leqslant m} \partial_h P_t\left( \sum\limits_j A_jx_j \right) (A_i)_{hk} \right) \sum\limits_s (A_i)_{ts} (x_i)_s \right.\right. \\
% & \left.\left. \hspace{24em} + P_t \left( \sum\limits_j A_jx_j \right) (A_i)_{tk} \right] \right)_{k=1}^{n_i} \\
% = & \operatorname{grad} c_i(x_i) - \left( \sum\limits_{1\leqslant t \leqslant m} \left[ \left( \sum\limits_{1\leqslant h \leqslant m} \partial_h P_t\left( Ax \right) (A_i)_{hk} \right) \sum\limits_s (A_i)_{ts} (x_i)_s + P_t \left( Ax \right) (A_i)_{tk} \right] \right)_{k=1}^{n_i}
% \end{align*}
% \endgroup

When $p$ is the linear inverse demand function $P(s) = p âˆ’ Ds$, where $p, s \in \mathbb{R}^m$, $D = \operatorname{diag}(d_1, \ldots, d_m) \in \operatorname{GL}_m(\mathbb{R})$. Then $\operatorname{Jac}(P)(s) = -D$. Let the local production cost functions be $c_i(x_i) = \pi_i \left(\sum\limits_{j=1}^{n_i} [x_i]_j\right)^2 + b_i^Tx_i$, then $\operatorname{grad} c_i(x_i) = 2\pi_i x_i + b_i$. Therefore,
$$
\operatorname{grad} g(x_i) = 2\pi_i x_i + b_i + A_i^TDA_ix_i - A_i^T \left( p - D\sum\limits_{1 \leqslant j \leqslant N} A_jx_j \right).
$$


\subsection*{Minimal Example}

Consider the networked Cournot game where there is one market with two companies. Let the price function of the market be $p(s) = 4 - s$ where $s$ is the supply. Let the production cost functions for the two companies be identical: $c(x_i) = x_i^2 + x_i$, $i = 1, 2$. Then the objective function for the companies are
$$\begin{cases}
\text{min} \ x_1^2 + x_1 - (4 - (x_1 + x_2)) x_1 \\
\text{min} \ x_2^2 + x_2 - (4 - (x_1 + x_2)) x_2 \\
\end{cases}$$
The solution is $x_1 = x_2 = 0.6.$


\printbibliography
