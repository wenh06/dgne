
\subsection*{Observations}

1. Problem Formulation: the Networked Cournot Game can be described by the following set of constrained optimization problems (ref. Equation (5) of \citep{Yi_2019}, note the difference in the last inequality constraints):
\begin{equation}
\label{eq:original_problem}
\def\arraystretch{1}
\begin{array}{rl}
\min & g(x_i) = f_i(x_i, \mathbf{x}_{-i}) = c_i(x_i) - (P(Ax))^TA_ix_i \\
\text{s.t.} & x_i \in \Omega_i \subsetneq \mathbb{R}^{n_i} \\
& Ax \leqslant r
\end{array}
\qquad i = 1, \ldots, N
\end{equation}
where
\begin{align*}
& c_i: \Omega_i \to \mathbb{R} \text{ is the local production cost function,} \\
& P: \mathbb{R}^m \to \mathbb{R}^m \text{ maps the total supply of each market to its corresponding price,}
\end{align*}
$A = [A_1, \ldots, A_N]$, $x = \operatorname{col}(x_1, \ldots, x_N)$, $Ax = \sum\limits_{1 \leqslant j \leqslant N} A_jx_j$, $\Omega_i$ the feasible set of $x_i$, usually a rectangle in the Euclidean space. The last constraint is called the shared affine coupling constraint.

This set of optimization problems can be reformulated as unconstrained optimization problems:
\begin{equation}
\label{eq:reformulated_problem}
\begin{array}{rl}
\min & g_i(x_i) + \langle \lambda_i, r-Ax \rangle + \iota_{\Omega_i} (x_i), \quad \lambda_i \in \mathbb{R}_-^m
\end{array}
\end{equation}
where $\iota_{\Omega_i} (x_i)$ is the indicator function of $\Omega_i$, whose subgradient is the normal cone $\mathcal{N}_{\Omega_i} (x_i)$ of $\Omega_i$, and $\operatorname{prox}_{\mathcal{N}_{\Omega_i}}(x_i) = \Pi_{\Omega_i}(x_i)$. Then applying (Douglas-Rachford? not the same) splitting, one gets the Algorithms (Algorithm 1 and 2) proposed in \citep{Yi_2019}.


2. It is stated in the numerical study section of \citep{Yu_2017} that ``\textcolor{red!60}{However, we rely on (110) in the simulations to examine the numerical performance regardless of solution feasibility.}'', where (110) is the constraints on the market capacities, similar to
\begin{align*}
A_i x_i \leqslant r_i, ~ \sum r_i \leqslant r ~ (\text{or } \sum r_i = r).
\end{align*}

In the first paragraph of section 3.2 of \citep{Yi_2019}, it is stated that ``\textcolor{red!60}{the shared affine coupling constraint is decomposed such that each player only knows a local block of the constraint matrix. Notice that $A_i$ characterizes how agent $i$ is involved in the coupling constraint (shares the global resource), also assumed to be privately known by player $i$. Then, the globally shared constraint $Ax \geqslant b$ couples the agents’ feasible decision sets, but is not known by any agent}''.

\begin{question}
What does it mean by ``each player only knows a local block of the constraint matrix''?
\end{question}

If one changes the original problem \eqref{eq:original_problem} to
\begin{equation}
\label{eq:new_problem}
\def\arraystretch{1}
\begin{array}{rl}
\min & g(x_i) = f_i(x_i, \mathbf{x}_{-i}) = c_i(x_i) - (P(Ax))^TA_ix_i \\
\text{s.t.} & x_i \in \Omega_i \subsetneq \mathbb{R}^{n_i} \\
& A_ix_i \leqslant r_i \\
& \sum r_i = r
\end{array}
\qquad i = 1, \ldots, N
\end{equation}
Then the reformulated unconstrained problem is
\begin{equation}
\label{eq:reformulated_new_problem}
\begin{array}{rl}
\min & g_i(x_i) + \langle \lambda_i, r_i-A_ix_i \rangle + \langle \mu_i, r - \sum r_i \rangle + \iota_{\Omega_i} (x_i), \quad \lambda_i \in \mathbb{R}_-^m
\end{array}
\end{equation}


3. In the last paragraph of section 3.2 of \citep{Yi_2019}, it is stated that ``\textcolor{red!60}{The update of $z_i$ in Algorithm 1 can be regarded as the discrete-time integrator for the consensual errors of local multipliers, which will ensure the consensus of $\lambda_i$ eventually}''. Since the authors stated that ``In this work, we seek a GNE with the same Lagrangian multiplier for all the agents, called variational GNE'' in section 3.1

\begin{question}
What is the situation for problems where the multipliers do not need a consensus?
\end{question}

TO ADD MORE ...

\subsection*{Computation of Gradients}

Computation of the gradient of the function in equation (36) in \citep{Yi_2019}

Consider the objective function
$$g(x_i) = f_i(x_i, \mathbf{x}_{-i}) = c_i(x_i) - (P(Ax))^T A_ix_i,$$

Let $p_i: \mathbb{R}^{n_i} \to \mathbb{R}^m$ be the function of supply of the $i$-th company to the markets, i.e. $p_i(x_i) = P(Ax) = P\left(\sum\limits_{1 \leqslant j \leqslant N} A_jx_j\right)$. Then
\begin{align*}
\operatorname{grad} g(x_i) & = \nabla_{x_i} f_i(x_i, \mathbf{x}_{-i}) = \operatorname{grad} c_i(x_i) - \left( \dfrac{\partial \left( (p_i(x_i))^T A_ix_i \right)}{\partial (x_i)_k} \right)_{k=1}^{n_i} \\
& = \operatorname{grad} c_i(x_i) - \left( \dfrac{\partial \left( \sum\limits_{1\leqslant t \leqslant m} (p_i(x_i))_t (A_ix_i)_t \right)}{\partial (x_i)_k} \right)_{k=1}^{n_i} \\
& = \operatorname{grad} c_i(x_i) - \left( \sum\limits_{1\leqslant t \leqslant m}\left( \dfrac{\partial \left( (p_i(x_i))_t \right)}{\partial (x_i)_k} (A_ix_i)_t + \dfrac{\partial \left( (A_ix_i)_t \right)}{\partial (x_i)_k} (p_i(x_i))_t \right) \right)_{k=1}^{n_i} \\
% & = \operatorname{grad} c_i(x_i) - \left( \sum\limits_{1\leqslant t \leqslant m}\left( \left(\operatorname{Jac} (p_i(x_i)) \right)_{tk} (A_ix_i)_t + \dfrac{\partial \left( \sum\limits_{s} (A_i)_{ts} (x_i)_s \right)}{\partial (x_i)_k} (p_i(x_i))_t \right) \right)_{k=1}^{n_i} \\
& = \operatorname{grad} c_i(x_i) - \left( \sum\limits_{1\leqslant t \leqslant m}\left( \left(\operatorname{Jac} (p_i)(x_i) \right)_{tk} (A_ix_i)_t + (A_i)_{tk} (p_i(x_i))_t \right) \right)_{k=1}^{n_i} \\
& = \operatorname{grad} c_i(x_i) - \left( \langle \operatorname{Jac}(p_i)(x_i)_{[:,k]}, A_ix_i \rangle + \langle (A_i)_{[:,k]}, p_i(x_i) \rangle \right)_{k=1}^{n_i} \\
& =\operatorname{grad} c_i(x_i) - \left( \operatorname{Jac}(p_i)(x_i) \right)^T A_ix_i - A_i^T p_i(x_i),
\end{align*}
with $\operatorname{Jac}(p_i)(x_i) = \left( \operatorname{Jac}(P)\left(\sum\limits_j A_jx_j\right) \right) \cdot A_i$.


When $p$ is the linear inverse demand function $P(s) = p − Ds$, where $p, s \in \mathbb{R}^m$, $D = \operatorname{diag}(d_1, \ldots, d_m) \in \operatorname{GL}_m(\mathbb{R})$. Then $\operatorname{Jac}(P)(s) = -D$. Let the local production cost functions be $c_i(x_i) = \pi_i \left(\sum\limits_{j=1}^{n_i} [x_i]_j\right)^2 + b_i^Tx_i$, then $\operatorname{grad} c_i(x_i) = 2\pi_i \left(\sum\limits_{j=1}^{n_i} [x_i]_j\right) + b_i$. Therefore,
\begin{align*}
\operatorname{grad} g(x_i) & = \left(2\pi_i \left(\sum\limits_{j=1}^{n_i} [x_i]_j\right) + b_i \right) + \left( A_i^TDA_ix_i - A_i^T \left( p - D\sum\limits_{1 \leqslant j \leqslant N} A_jx_j \right) \right)
\end{align*}

The expression of $\operatorname{Jac}(p_i)(x_i)$ corresponds to the function \texttt{market\_price\_jac} in the file \texttt{python/simulation.py}, the expression of $\operatorname{grad} g(x_i)$ corresponds to the attribute \texttt{\_objective\_grad} of the \texttt{Company} class in \texttt{python/networked\_cournot\_game.py}.


\subsection*{Minimal Example}

Consider the networked Cournot game where there is one market with two companies. Let the price function of the market be $p(s) = 4 - s$ where $s$ is the supply. Let the production cost functions for the two companies be identical: $c(x_i) = x_i^2 + x_i$, $i = 1, 2$. Then the objective function for the companies are
$$\begin{cases}
\text{min} \ x_1^2 + x_1 - (4 - (x_1 + x_2)) x_1 \\
\text{min} \ x_2^2 + x_2 - (4 - (x_1 + x_2)) x_2 \\
\end{cases}$$
The solution is $x_1 = x_2 = 0.6.$


\printbibliography
